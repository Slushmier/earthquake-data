---
title: "Written Assignment Answers"
author: "Adam Armeier"
date: "6/26/2022"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readxl)
library(sf)
library(spdep)
library(rvest)
library(knitr)

# Read in provided earthquake file
earthquakes <- read_xlsx("Data\\Data Set - Earthquakes.xlsx")[-1, ] %>% 
  dplyr::filter(is.na(Longitude) == FALSE) %>% 
  st_as_sf(coords = c('Longitude', 'Latitude'), remove = F, crs = 4326)

# Output file from embassy_scrape.R
embassies <- read_csv("Data\\embassies_consulates.csv")

# Read in Natural Earth populate places file to get coordinates of cities
# Many manual naming changes to ensure the webscraped embassies match the
# spelling in the Natural Earth file
pop_places <- st_read("Data\\ne_10m_populated_places_simple\\ne_10m_populated_places_simple.shp", crs=4326) %>%
  dplyr::mutate(nameascii = as.character(nameascii)) %>% 
  dplyr::filter(sov0name != "United States") %>% 
  dplyr::mutate(nameascii = 
                  case_when(
                    nameascii == "Kobenhavn" ~ "Copenhagen",
                    nameascii == "Port-of-Spain" ~ "Port of Spain",
                    nameascii == "Tel Aviv-Yafo" ~ "Tel Aviv",
                    nameascii == "Basra" ~ "Basrah",
                    nameascii == "Willemstad" ~ "Curacao",
                    nameascii == "Az Zahran" ~ "Dhahran",
                    nameascii == "Dusseldorf" ~ "DÃ¼sseldorf",
                    nameascii == "Guatemala" ~ "Guatemala City",
                    nameascii == "Kiev" ~ "Kyiv",
                    nameascii == "Palikir" ~ "Kolonia",
                    nameascii == "Shenyeng" ~ "Shenyang",
                    TRUE ~ nameascii
                  )) %>% 
  dplyr::filter(featurecla != "Populated place" | 
                  nameascii == "Ciudad Juarez" | 
                  nameascii == "Montreal" |
                  nameascii == "Calgary" |
                  nameascii == "Dhahran" | 
                  nameascii == "Erbil" |
                  nameascii == "Frankfurt" |
                  nameascii == "Jeddah" |
                  nameascii == "Leipzig" |
                  nameascii == "Matamoros" |
                  nameascii == "Nogales" |
                  nameascii == "Nuevo Laredo" |
                  nameascii == "Tijuana" |
                  nameascii == "Vancouver") %>% 
  dplyr::filter(!(nameascii == "La Paz" & 
                    (sov0name == "Mexico" | sov0name == "Honduras"))) %>% 
  dplyr::filter(!(nameascii == "Santiago" & 
                    (sov0name == "Panama" | 
                       sov0name == "Dominican Republic")))  %>% 
  dplyr::filter(!(nameascii == "Hamilton" & 
                    (sov0name == "New Zealand"))) %>% 
  dplyr::filter(!(nameascii == "Merida" & 
                    (sov0name == "Venezuela" | adm0name == "Spain"))) %>% 
  dplyr::filter(!(nameascii == "Tripoli" & 
                    (sov0name == "Greece")))

# Join the populated places and embassies file
sf_df <- left_join(pop_places, 
                   embassies, 
                   by = c("nameascii" = "Location")) %>% 
  dplyr::filter(!is.na(Type)) %>% 
  arrange(nameascii) %>% 
  select(name, namealt, nameascii, sov0name,
         adm0name, latitude, longitude, Country, Type)

# Created a global hex-grid with unique indexes in each hexagon
sfc_global <- st_bbox(pop_places) %>% st_as_sfc()
global_grid <- st_make_grid(sfc_global, 2, crs = st_crs(sf_df), square = F)
global_grid <- st_sf(hexid = 1:length(lengths(global_grid)), global_grid)

# Get the intersection between the hexagons and earthquake events
hexid <- sapply(st_intersects(x = earthquakes, y = global_grid), 
                function(z) if (length(z)==0) NA_integer_ else z[1])
earthquakes$hexid <- hexid

# Provide summary statistics for each hexagon
earthquake_hexsum <- earthquakes %>% group_by(hexid) %>% 
  summarize(events = n(),
            events_maj = sum(Mag >= 7),
            Deaths = sum(Deaths, na.rm = T),
            Injuries = sum(Injuries, na.rm = T)) %>% 
  st_set_geometry(NULL) 

# Join the initial grid file and the summarized statistics
quake_grid <- left_join(global_grid, earthquake_hexsum) %>% 
  replace_na(list(events = 0, Deaths = 0,
                  Injuries = 0, events_maj = 0)) %>% 
  mutate(Casualties = Deaths + Injuries)

# More complicated analysis based upon regional effects
neighbors_grid <- poly2nb(global_grid, queen = T)
grid_w <- nb2listw(neighbors_grid, style = "W")
quake_grid$local_g <- as.numeric(localG(quake_grid$events_maj,
                                        grid_w, zero.policy = TRUE))
quake_grid$local_i <- as.numeric(localmoran(quake_grid$events, grid_w)[,4])
# Moran's I test to demonstrate high spatial autocorrelation
# moran.test(quake_grid$events, grid_w)

# Calculate risk based on the number of events and deaths in an area
quake_grid <- quake_grid %>% 
  mutate(risk = scale(log(events_maj+1)) + scale(log(Deaths + 1)) +
           local_g,
         risk_lvl = ifelse(risk < 10, 1,
                           ifelse(risk < 20, 2, 3))) %>% 
  dplyr::filter(risk >= 5)

# Get the intersection between the quake grid and the embassies file
sf_df$hexid <- sapply(st_intersects(x = sf_df, y = global_grid),
                        function(z) if (length(z)==0) NA_integer_ else z[1])
quake_grid_simple <- quake_grid
st_geometry(quake_grid_simple) <- NULL
embassy_risk <- left_join(sf_df, quake_grid_simple) %>% 
  replace_na(list(risk = 0, risk_lvl = 0)) %>% 
  rename(`Mission Risk` = risk_lvl)

quake_grid <- quake_grid %>% rename(`Area Risk` = risk_lvl) %>% 
  mutate(`Area Risk` = as.factor(`Area Risk`))
```

## Question 1 - Study Design

**Please discuss in detail your approach to this analysis.  Study Design may Include how you defined the question, data, methodology, and analytic design or your methodology to data exploration and explanation, the assumptions taken, how you would ensure the integrity and accuracy of any results, and any considerations for communication to a varied audience.** 

I defined the question directly from the provided scenario, which asks for 
a "rapid preliminary analysis of the risk of earthquakes to our overseas
missions." Therefore, I consider the research questions to be 
straightforward from the scenario: "What is the risk of earthquakes to 
State Department overseas missions?" This requires defining a method to
measure risk of earthquakes and comparing it for each of the State 
Department's overseas missions. 

I choose to use a method that defines the relative, rather than the absolute
risk for the State Department's missions. My assumption is that this 
preliminary assessment is to provide decision makers with information on  
which missions are most at risk, rather than provide a likelihood that a 
particular mission will be affected by a serious earthquake. To provide an
absolute risk, I would probably develop a logistical regression 
model to attempt to quantify the risk based upon a number of factors 
related to earthquake risk. However, since I know very little about 
earthquake risk and know that earthquake prediction has not been very 
succesful, I rely on a straightforward assessment based on the provided 
earthquake spreadsheet.

Although not feasible for this scenario, I would also try to clarify  
how the person or organization making the request defines risk. I would 
likely try to communicate with the customer to ensure that I understand 
their desired output. For example, do they perceive the risk of earthquakes 
to simply be a risk of an earthquake event occurring, or does it also need 
to account for factors such as the capabilities of a host nation to provide 
disaster relief support, the ability of the Department of State or other US 
government support entities to access the mission in an emergency, or the 
risk of factors like tidal waves that may result from an earthquakes? I 
also looked at the Joint Strategic Plan for guidance on the type of risk 
assessment mentioned. Given the text from the prompt and Performance Goal 
4.3.4 to "protect personnel through advanced building" I assume the goal is
to prioritize which missions are most at risk and may need to be rebuilt or
reinforced in order to protect the personnel stationed there. Hence, I use
a relative risk assessment.

I use a spatial approach of analysis to account for variations in the 
threat level to consulates within countries where the State Department may 
have multiple consulates. For example, analyzing data for the United States 
would provide a much different earthquake risk when comparing, for example, 
California to Illinois. The spatial methodology allows for accounting for 
the variation within various countries, particularly those with consulates 
outside of the capital that may have varying risks to earthquakes. The 
features in the dataset that are the most complete are the time, the 
geographic coordinates, the magnitude, and the focal depth. Otherwise, most
of the spreadsheet contains missing data for about half of the features.

I initially compiled a geographic index that accounts for the frequency of 
events within certain areas and the number of fatalities that occurred 
because of the events reported. One flaw of using the provided dataset is 
that it doesn't account well for low likelihood, e.g. a 1 in 500 year event.
The 52 years of earthquake data is definitely a good guide for extreme 
events, but may not be enough history to predict if an area is overdue for 
an extreme event that hasn't happened and, therefore, is not in the dataset.

Given the extension to submit these results, I decided to also use a spatial
clustering method to account for regional risks and, hopefully, negate any
effects related to extreme events falling outside of the aggregate areas 
simply because of random chance.

A primary reason that I use a simple index is to ensure that the results are
easily explicable to a senior policymaker, particularly if they are 
presented as one simple graphic. Since the prompt suggests the Secretary is
requesting assessments about multiple natural disasters, I assume that there
would be limited time to discuss the results and the methods that went into
this particular earthquake assessment. I presume the assessment would along 
other assessments like hurricane risk, flooding risk, and other natural 
disaster risks. A simple index would likely be the best way to summarize
the data analysis.

## Question 2 - Data Analysis

**What are key findings found in your analysis? Which embassies are most at risk? Are there groups of embassies that share similar features or profiles with respect to earthquake risk? Were there outliers or areas with data quality issues? Please describe how you conducted your data analysis, including which tools you used, why you chose them, and data preprocessing steps you took, and justify the results of your analysis.**   

I find that the embassies and consulates most at risk of being affected by 
an earthquake are along or near a Pacific Coast or in areas of historically 
bad earthquakes. Manila is identified as the most at risk, followed by 
Quito, and Port-au-Prince. Port-au-Prince ranks so high because of how 
devastating the 2010 earthquake was.

```{r echo=FALSE}
subset_emb <- embassy_risk %>% slice_max(risk, n = 10) %>% 
  arrange(desc(risk)) %>% 
  select(name, sov0name, risk, `Mission Risk`) %>% 
  rename(City = name, Country = sov0name,
         Risk = risk, `Risk Level` = `Mission Risk` )
st_geometry(subset_emb) <- NULL

kable(subset_emb)
```

I decided to utilize a relatively simple model because there 
was so much categorical data missing for many of the events. Only the 
location description, dates, coordinates, magnitude, and focal depth were 
available for almost all of the events, despite there being 38 columns of 
data. I chose to use the categories for *Deaths* to *Injuries* 
to measure try and have a proxy measure of the relative danger of 
earthquakes in specific areas compared to others. Despite these variables 
having many missing values, a quick visualization of the location of events 
with no recorded deaths or injuries showed that they were often over water 
or in sparsely populated areas. This provided some assurance that the 
missing data actually reflected a 0 value, rather than simply being missing
or uncollected data.

I decided against using nay of the measures of economic damage, or loss of
houses due to concerns about the number of missing attributes and concerns
that the global data collection process across various countries and time
would lead to inconsistently quantified data within the dataset.

I used a small-scale geographic analysis of the earthquake dataset in order
to depict the results at a global scale. I generated a global hexagonal grid
and then aggregated the attributes for each event into the individual 
polygon grid spaces. The aggregated variables I used were deaths, injuries,
number of events, and number of extreme events (magnitude >= 7). I initially
compiled an index based on on the number of deaths from all events in the
dataset and the number of earthquakes in the same areas in the dataset. 
However, one of the initial problems I recognized with this form of analysis
was that example of the modified areal unit problem, where the level of 
aggregation of point events into polygon areas can create results that
reflect the data aggregation process more than the real-world phenomenon
that we're trying to explain through the data analysis.

Given the extension we had, I added one aspect to the risk index, which was
a measure of regional event clustering. This was an attempt to account for 
the fact that some extreme events may have been aggregated into neighboring 
polygons to the areas that were effected. For example, Kathmandu was 
relatively low in my initial index because the major 2015 earthquake that 
affected the city was fairly distant from the city and the Embassy within
it, despite having a major effect. By adding a measure of regional 
clustering, the Embassy was in the top 15, which seemed to provide a clearer
reflection of what had happened.

To measure the regional clustering, I use the Getis-Ord G* statistic for
measuring local spatial autocorrelation. The statistic provides a z-score
of the likelihood that an area is in a nonrandom clustered area for a 
specific phenomenon. For this exercise, the statistic reflects the 
likelihood that a spatial hexagon, together with its six neighbors, 
represent a cluster of high seismic activity. This way, even an area with 
no measured seismic activity because of random variation, will still be 
reflected as being in a cluster of high activity if its neighbors show 
high levels.

Getting the locations of the Embassies and Consulates required a couple
false starts before developing a dataset to support this analysis by 
using some web-scraped data. Initially, I tried to acquire the geographic
data for Embassies and Consulates by going to OpenStreetMap and getting the
data for US diplomatic buildings. However, the final result from this 
source was missing over half of the Embassies and Consulates. After that, I 
decided to use a web-scraping tool in R, the rvest package, to get the data
for active Embassies and Consulates listed on the https://www.usembassy.gov/
website. Since all the Embassies and Consulates aren't listed on the main 
page, I scraped a list of unique URLs for each country and then scraped the
names of each Embassy and Consulate. I didn't gather the specific address
for each location as I don't personally have access to a global address 
geocoding service. With the list of unique Embassies and Consulates, I 
combined that dataset with a Natural Earth dataset for the coordinates of
global cities and towns. Most of the locations were easily found, though
I had to manually match some of the listed attributes in both of the 
datasets.

Although the dataset for Embassies was accurate only to the level of the 
city or town where the Embassy or Consulate was located, I thought that this
level of precision was acceptable for a global product. For more specific
data analysis, I would have liked to have more precise geographic 
coordinates and I would probably ask HIU or INR/GGI about that sort of data
set if working on a similar project for the State Department.

Finally, I chose to do my analysis in R, primarily using the tidyverse and 
sf packages. Although I could have done this analysis in ArcGIS, I wanted to
use R in order to accommodate better reproducability if other analysts 
(myself included) wanted to leverage this type of model for other
risk assessments. Given that this analysis is intended to be 
preliminary, it is likely to be modified and refined with more time. I also
find that R is generally more stable and faster to use than a more
traditional geospatial program. In addition, with more time on this project,
I would want to work with academic researchers or outside consultants that 
have a more holistic understanding of quantifying earthquake risk. I find 
that using GitHub provides a good way to share the analysis and data
with other potential collaborators. For example, I have uploaded my files 
to GitHub if you would like to see the example of the code that I used for
this document (https://github.com/Slushmier/earthquake-data).

## Question 3 - Communication

**Please describe in detail how you would communicate your results to a diverse, non-technical audience. Provide an executive summary of your analysis for senior management which does not exceed 500 words.** 

Wherever plausible, I try to add a visual element in order to frame a 
conversation with an executive. For this particular instance, I provided a
map that showed the highest risk Embassies or Consulates, and the relative
global risk that I assessed from the initial earthquake file. I would try
to provide a graphic that shows the results provided in a graphic form.

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=7.5}
library(ggrepel)

# Natural Earth country boundaries
countries <- st_read("Data\\ne_110m_admin_0_countries\\ne_110m_admin_0_countries.shp", crs = 4326)

plot_embassy <- embassy_risk %>% 
  select(nameascii, risk, `Mission Risk`) %>% 
  mutate(icon_size = `Mission Risk`) %>% 
  mutate(`Mission Risk` = case_when(
    `Mission Risk` == 0 ~ "Minimal",
    `Mission Risk` == 1 ~ "Low",
    `Mission Risk` == 2 ~ "Moderate",
    `Mission Risk` == 3 ~ "High")) %>% 
  mutate(shape = 18)

top_10_embassies <- embassy_risk %>% 
  arrange(desc(risk)) %>% 
  slice_max(order_by = risk, n = 10) %>% 
  mutate(shape = 19)
  
ggplot() + 
  geom_sf(data = countries, fill = "lightgray", alpha = 0.2) +
  geom_sf(data = quake_grid,
          aes(fill = `Area Risk`),
          alpha = 0.5, color = NA) +
  scale_fill_brewer(name = "Area Earthquake Risk", 
                    breaks = c(1, 2, 3),
                    labels = c("Low", "Moderate", "High"),
                    palette = "YlOrRd") +
  geom_sf(data = plot_embassy, aes(shape = as.factor(shape)),
          size = 0.75, alpha = 0.8, color = "black",
          name = "embassy") +
  geom_sf(data = top_10_embassies, aes(shape = as.factor(shape)),
          size = 2, alpha = 0.9, color = "dark red") +
  ggrepel::geom_text_repel(data = top_10_embassies,
    aes(label = nameascii, geometry = geometry),
    stat = "sf_coordinates", min.segment.length = 0,
    bg.color = "white", bg.r = 0.25, alpha = 0.8, size = 3) +
  theme_minimal() +
  ggtitle("Highest Earthquake Risk for Embassies and Consulates") +
  theme(legend.position = "bottom",
        plot.title = element_text(hjust = 0.5)) +
  scale_shape_manual(name = NULL, values = c(19, 18),
                     labels = c("Highest Risk Missions", "Other Missions"),
                     guide = guide_legend(override.aes =                                             list(color = c("dark red", "black"),
                                                                                                          size = c(2, 0.75),
                              shape = c(19, 18)  ))) +
  xlab(NULL) + ylab(NULL)
  
  
### Check that zero death & injury events are remote
# earthquakes_sub <- earthquakes %>% 
#  dplyr::filter(is.na(Deaths) & is.na(Injuries))
# ggplot() + geom_sf(data = countries) + 
#  geom_sf(data = earthquakes_sub, aes(color = Mag))
```

**Executive Summary**

We assessed the relative risk of earthquakes for all U.S. Embassies and 
Consulates. The highest risk was found to exist for five missions in Latin
America close to the Pacific Coast; Port-au-Prince, Haiti; Mexico City, 
Mexico; Tbilisi, Georgia; Peshawar, Pakistan; Manila, Philippines; and 
Beijing, China. We modeled global earthquake risk based on the historic
presence of high magnitude earthquakes, the historic presence of earthquakes
with high casualties, and regions of high seismic activity. We created a 
combined risk assessment for each area of the world based on these three
primary factors. Of the 240 global embassies and consulates we analyzed,
seven were in areas of high earthquake risk, 15 were in areas of 
moderate risk, 18 were in areas of low risk, and 200 were in areas of 
minimal risk.

## Question 4 - Validation

**Hypothetically, if a member of your team completed this work using software or methodology in which you are not proficient, describe how you would validate their work.** 

If I don't know the software of methodology that a team member has used, I
would most likely ask questions about the research design that was used to 
try and analyze the particular question. Asking questions such as what the
main research question was, why a particular dataset was used for that 
research question, and why an analyst used a particular software or 
methodology would help to provide some validation about the work that was
done.

If the analytic methodology were particularly complex, I would also ask 
about data processing, such as accounting for missing values, inconsistent 
annotations, outliers, and potential duplicated results. If the method of 
analysis is particularly complex for a relatively simple issue, I would 
probably ask about the use of a simulated dataset to validate that the 
method of analysis produced logical results.

Hopefully much of the code would be available in a git repository and, if 
so, I could skim the code for any obvious issue, or try to send the code to 
a colleague I know that might be more familiar with the software or process.
I would also do a quick internet search for any other similar analyses, 
simply to ensure that any results make logical sense.